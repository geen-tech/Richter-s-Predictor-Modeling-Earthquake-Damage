{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e59a6c24",
   "metadata": {},
   "source": [
    "# Random Search e Nested Cross-Validation per ottimizzare gli iperparametri del classificatore LightGBM"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7eb2937",
   "metadata": {},
   "source": [
    "# Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5ff24c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "import optuna\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "722415ee",
   "metadata": {},
   "source": [
    "# Caricamento dati di training "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93a6bf7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "BASE = Path(__file__).parent\n",
    "X = pd.read_csv(BASE / 'train_values.csv', index_col=0)\n",
    "y_df = pd.read_csv(BASE / 'train_labels.csv', index_col=0)\n",
    "y = y_df['label'] if 'label' in y_df.columns else y_df.iloc[:, 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951a2b59",
   "metadata": {},
   "source": [
    "# Parte di preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "597e4099",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
    "for c in cat_cols:\n",
    "    X[c] = X[c].astype('category')\n",
    "\n",
    "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
    "i = SimpleImputer(strategy='median')\n",
    "X[num_cols] = i.fit_transform(X[num_cols])\n",
    "\n",
    "# Clipping dei valori anomali ai percentili 1° e 99°\n",
    "# Serve a limitare l'influenza degli outlier estremi nelle variabili numeriche\n",
    "for col in num_cols:\n",
    "    lo, hi = X[col].quantile(0.01), X[col].quantile(0.99)\n",
    "    X[col] = X[col].clip(lo, hi)\n",
    "\n",
    "# Aggiunge una nuova feature che indica il numero di valori mancanti per ogni riga\n",
    "# Utile per informare il modello su possibili anomalie nei dati\n",
    "X['missing_count'] = X.isnull().sum(axis=1)\n",
    "\n",
    "# Imposta il trasformatore di colonne per il preprocessing delle variabili numeriche e categoriche\n",
    "dct = ColumnTransformer([\n",
    "    ('num', SimpleImputer(strategy='median'), num_cols),     \n",
    "    ('scale', StandardScaler(), num_cols),                  \n",
    "    ('ohe', OneHotEncoder(handle_unknown='ignore', sparse_output=False), cat_cols) \n",
    "])\n",
    "\n",
    "# Applica il preprocessing ai dati di training\n",
    "X_proc = dct.fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9135e3",
   "metadata": {},
   "source": [
    " # Nested-Cross-validation per la ricerca degli iperparametri migliori "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8f288f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTER_K, INNER_K, N_TRIALS = 5, 3, 40\n",
    "outer_cv = StratifiedKFold(n_splits=OUTER_K, shuffle=True, random_state=42)\n",
    "inner_cv = StratifiedKFold(n_splits=INNER_K, shuffle=True, random_state=2025)\n",
    "\n",
    "# Definisce la funzione obiettivo per Optuna per l'ottimizzazione degli iperparametri di LightGBM\n",
    "def objective(trial, X_tr, y_tr):\n",
    "    params = {\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 700, 1300),           # Numero di alberi nella foresta\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1),       # Tasso di apprendimento\n",
    "        'num_leaves': trial.suggest_int('num_leaves', 120, 200),                # Numero massimo di foglie in un albero\n",
    "        'min_child_samples': trial.suggest_int('min_child_samples', 30, 100),   # Minimo numero di dati in una foglia\n",
    "        'subsample': trial.suggest_float('subsample', 0.7, 1.0),                # Percentuale di campioni usati per ciascun albero\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.5, 1.0),  # Percentuale di colonne usate per ciascun albero\n",
    "        'feature_fraction': trial.suggest_float('feature_fraction', 0.6, 0.9),  # Percentuale di feature da considerare in ogni iterazione\n",
    "        'reg_alpha': trial.suggest_float('reg_alpha', 0.2, 0.8),                # Regolarizzazione L1\n",
    "        'reg_lambda': trial.suggest_float('reg_lambda', 0.01, 0.8),             # Regolarizzazione L2\n",
    "        'random_state': 42,                                                     # Semina per la riproducibilità\n",
    "        'n_jobs': -1,                                                           # Usa tutti i core disponibili\n",
    "        'verbose': -1                                                           # Disattiva output verboso\n",
    "    }\n",
    "    scores = []\n",
    "    # Cross-validation interna per valutare le performance dei parametri selezionati\n",
    "    for tr_idx, va_idx in inner_cv.split(X_tr, y_tr):\n",
    "        X_t, X_v = X_tr[tr_idx], X_tr[va_idx]\n",
    "        y_t, y_v = y_tr[tr_idx], y_tr[va_idx]\n",
    "        model = LGBMClassifier(**params)\n",
    "        model.fit(X_t, y_t)\n",
    "        preds = model.predict(X_v)\n",
    "        scores.append(f1_score(y_v, preds, average='micro'))\n",
    "    return np.mean(scores)\n",
    "\n",
    "# Raccolta metriche\n",
    "metrics_per_fold = {\n",
    "    'accuracy': [],\n",
    "    'precision_macro': [],\n",
    "    'recall_macro': [],\n",
    "    'f1_micro': [],\n",
    "    'f1_macro': [],\n",
    "}\n",
    "\n",
    "# Nested CV\n",
    "for fold, (tr_idx, va_idx) in enumerate(outer_cv.split(X_proc, y), 1):\n",
    "    print(f\"\\nFold {fold}/{OUTER_K}\")\n",
    "    X_tr, X_va = X_proc[tr_idx], X_proc[va_idx]\n",
    "    y_tr, y_va = y[tr_idx], y[va_idx]\n",
    "\n",
    "    study = optuna.create_study(direction='maximize')\n",
    "    study.optimize(lambda t: objective(t, X_tr, y_tr), n_trials=N_TRIALS)\n",
    "\n",
    "    best = study.best_params\n",
    "    model = LGBMClassifier(**best)\n",
    "    model.fit(X_tr, y_tr)\n",
    "    y_pred = model.predict(X_va)\n",
    "\n",
    "    acc = accuracy_score(y_va, y_pred)\n",
    "    prec = precision_score(y_va, y_pred, average='macro', zero_division=0)\n",
    "    rec = recall_score(y_va, y_pred, average='macro', zero_division=0)\n",
    "    f1_mi = f1_score(y_va, y_pred, average='micro')\n",
    "    f1_ma = f1_score(y_va, y_pred, average='macro')\n",
    "\n",
    "    metrics_per_fold['accuracy'].append(acc)\n",
    "    metrics_per_fold['precision_macro'].append(prec)\n",
    "    metrics_per_fold['recall_macro'].append(rec)\n",
    "    metrics_per_fold['f1_micro'].append(f1_mi)\n",
    "    metrics_per_fold['f1_macro'].append(f1_ma)\n",
    "\n",
    "    print(f\"Acc: {acc:.4f} | Prec_macro: {prec:.4f} | Recall_macro: {rec:.4f} | F1_micro: {f1_mi:.4f} | F1_macro: {f1_ma:.4f}\")\n",
    "\n",
    "# Report finale\n",
    "print(\"\\n=== Risultati medi sui fold esterni ===\")\n",
    "for metric, values in metrics_per_fold.items():\n",
    "    print(f\"{metric}: {np.mean(values):.4f} ± {np.std(values):.4f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
