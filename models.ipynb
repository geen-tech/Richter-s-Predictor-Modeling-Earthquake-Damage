{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c599b2f0",
   "metadata": {},
   "source": [
    "# Modelli: training con RandomForest e LightGBM in ensemble"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b15243a6",
   "metadata": {},
   "source": [
    "# Imports "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a68b4691",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from lightgbm import LGBMClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4db91a9a",
   "metadata": {},
   "source": [
    "# Parte Princiaple\n",
    "La funzione: applica una Stratified K-Fold CV per evitare bias in problemi sbilanciati, allena due modelli (RandomForestClassifier e LGBMClassifier) su ciascun fold, crea un ensemble \"soft\" scegliendo, per ciascun esempio, la predizione del modello con la confidenza maggiore (valore massimo tra le probabilità), ritorna tutte le predizioni combinate da tutti i fold, utili per calcolare metriche globali."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f705baa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_ensemble(X, y):\n",
    "    # Inizializza un classificatore Random Forest con iperparametri specifici\n",
    "    rf = RandomForestClassifier(\n",
    "        n_estimators=953,\n",
    "        max_features=0.5,\n",
    "        min_samples_split=9,\n",
    "        min_samples_leaf=3,\n",
    "        bootstrap=True,\n",
    "        n_jobs=-1,\n",
    "        random_state=2025\n",
    "    )\n",
    "    # Inizializza un classificatore LightGBM con iperparametri ottimizzati\n",
    "    lgbm = LGBMClassifier(\n",
    "        n_estimators=800,\n",
    "        learning_rate=0.06,\n",
    "        num_leaves=160,\n",
    "        max_depth=None,\n",
    "        min_child_samples=60,\n",
    "        subsample=0.75,\n",
    "        colsample_bytree=0.5,\n",
    "        reg_alpha=0.5,\n",
    "        reg_lambda=0.026,\n",
    "        verbosity=-1,\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    # Imposta una cross-validation stratificata a 5 fold\n",
    "    kf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2025)\n",
    "    # Lista per raccogliere tutte le predizioni del processo di validazione\n",
    "    preds = []\n",
    "     # Loop su ciascun fold della cross-validation\n",
    "    for train_idx, val_idx in kf.split(X, y):\n",
    "        X_tr, X_val = X[train_idx], X[val_idx]\n",
    "        y_tr, y_val = y.iloc[train_idx], y.iloc[val_idx]\n",
    "        # Addestra entrambi i modelli sul fold corrente\n",
    "        rf.fit(X_tr, y_tr)\n",
    "        lgbm.fit(X_tr, y_tr)\n",
    "        # Calcola le probabilità di classe per il set di validazione\n",
    "        proba_rf = rf.predict_proba(X_val)\n",
    "        proba_lgbm = lgbm.predict_proba(X_val)\n",
    "        fold_preds = []\n",
    "        # Ensemble delle predizioni: seleziona il modello con la probabilità massima più alta\n",
    "        for i in range(len(X_val)):\n",
    "            p_rf = proba_rf[i]\n",
    "            p_lg = proba_lgbm[i]\n",
    "            # Estrae la classe predetta da ciascun modello\n",
    "            pred_rf = rf.classes_[np.argmax(p_rf)]\n",
    "            pred_lg = lgbm.classes_[np.argmax(p_lg)]\n",
    "            # Aggiunge la predizione del modello più sicuro\n",
    "            fold_preds.append(pred_lg if np.max(p_lg) > np.max(p_rf) else pred_rf)\n",
    "        preds.extend(fold_preds)\n",
    "    # Ritorna tutte le predizioni ottenute in validazione durante la cross-validation\n",
    "    return preds\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
