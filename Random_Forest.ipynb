{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOCfNzNTfT9cpTasEmToBq0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/geen-tech/Richter-s-Predictor-Modeling-Earthquake-Damage/blob/main/Random_Forest.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Random Forest Classifier\n",
        "Questo notebook implementa un modello Random Forest per la previsione dei danni strutturali. Include preprocessing, training e salvataggio delle predizioni."
      ],
      "metadata": {
        "id": "IG9PJgIT2jK1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Imports"
      ],
      "metadata": {
        "id": "B0PsUFMU2yc0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "import sys\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.feature_selection import VarianceThreshold\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.model_selection import cross_validate, StratifiedKFold\n",
        "from sklearn.metrics import make_scorer, accuracy_score, f1_score, precision_score, recall_score\n"
      ],
      "metadata": {
        "id": "CZPaXO172zv5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Progress Bar Helper"
      ],
      "metadata": {
        "id": "dnCC0r083QkW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def progress(p):\n",
        "    sys.stdout.write(f\"\\rAvanzamento: {p:3d}%\")\n",
        "    sys.stdout.flush()\n",
        "    if p >= 100:\n",
        "        sys.stdout.write(\"\\n\")"
      ],
      "metadata": {
        "id": "-7QH6e_23aIa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Load Data"
      ],
      "metadata": {
        "id": "vi4sW4ZD3hPp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "BASE = Path().resolve()\n",
        "progress(0)\n",
        "X = pd.read_csv(BASE / 'train_values.csv', index_col='building_id')\n",
        "y = pd.read_csv(BASE / 'train_labels.csv', index_col='building_id')['damage_grade']\n",
        "Xt = pd.read_csv(BASE / 'test_values.csv', index_col='building_id')\n",
        "progress(10)"
      ],
      "metadata": {
        "id": "6i2YHayy3lAz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Preprocessing"
      ],
      "metadata": {
        "id": "ep0lgfYn3rqG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "cat_cols = X.select_dtypes(include='object').columns.tolist()\n",
        "for c in cat_cols:\n",
        "    X[c] = X[c].astype('category')\n",
        "    if c in Xt.columns:\n",
        "        Xt[c] = Xt[c].astype('category')\n",
        "\n",
        "nunique = X.nunique()\n",
        "low_var = nunique[nunique <= 1].index.tolist()\n",
        "if low_var:\n",
        "    X.drop(columns=low_var, inplace=True)\n",
        "    Xt.drop(columns=low_var, inplace=True, errors='ignore')\n",
        "    cat_cols = [c for c in cat_cols if c not in low_var]\n",
        "\n",
        "num_cols = X.select_dtypes(include=['int64', 'float64']).columns.tolist()\n",
        "imp = SimpleImputer(strategy='median')\n",
        "X_num = pd.DataFrame(imp.fit_transform(X[num_cols]), columns=num_cols, index=X.index)\n",
        "Xt_num = pd.DataFrame(imp.transform(Xt[num_cols]), columns=num_cols, index=Xt.index)\n",
        "for col in num_cols:\n",
        "    lower = X_num[col].quantile(0.01)\n",
        "    upper = X_num[col].quantile(0.99)\n",
        "    X_num[col] = X_num[col].clip(lower, upper)\n",
        "    Xt_num[col] = Xt_num[col].clip(lower, upper)\n",
        "X_num['missing_count'] = X[num_cols].isnull().sum(axis=1)\n",
        "Xt_num['missing_count'] = Xt[num_cols].isnull().sum(axis=1)\n",
        "\n",
        "for a, b in [(1,2), (1,3), (2,3)]:\n",
        "    X_num[f'geo_sum_{a}{b}'] = X[f'geo_level_{a}_id'] + X[f'geo_level_{b}_id']\n",
        "    Xt_num[f'geo_sum_{a}{b}'] = Xt[f'geo_level_{a}_id'] + Xt[f'geo_level_{b}_id']\n",
        "    X_num[f'geo_prod_{a}{b}'] = X[f'geo_level_{a}_id'] * X[f'geo_level_{b}_id']\n",
        "    Xt_num[f'geo_prod_{a}{b}'] = Xt[f'geo_level_{a}_id'] * Xt[f'geo_level_{b}_id']\n",
        "\n",
        "eps = 1e-5\n",
        "X_num['geo_prod_123'] = X['geo_level_1_id'] * X['geo_level_2_id'] * X['geo_level_3_id']\n",
        "Xt_num['geo_prod_123'] = Xt['geo_level_1_id'] * Xt['geo_level_2_id'] * Xt['geo_level_3_id']\n",
        "for a, b in [(1,2), (1,3), (2,3)]:\n",
        "    X_num[f'geo_div_{a}{b}'] = X[f'geo_level_{a}_id'] / (X[f'geo_level_{b}_id'] + eps)\n",
        "    Xt_num[f'geo_div_{a}{b}'] = Xt[f'geo_level_{a}_id'] / (Xt[f'geo_level_{b}_id'] + eps)\n",
        "\n",
        "preprocessor = ColumnTransformer([\n",
        "    ('num', 'passthrough', X_num.columns.tolist()),\n",
        "    ('cat', OneHotEncoder(drop='first', handle_unknown='ignore', sparse_output=False), cat_cols)\n",
        "])\n",
        "\n",
        "Xp = preprocessor.fit_transform(pd.concat([X_num, X[cat_cols]], axis=1))\n",
        "Xt_p = preprocessor.transform(pd.concat([Xt_num, Xt[cat_cols]], axis=1))\n",
        "\n",
        "progress(30)"
      ],
      "metadata": {
        "id": "wk2fBDLq3wNx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Cross-Validation Metrics"
      ],
      "metadata": {
        "id": "Gsv06UGd4Gb0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=2025)\n",
        "scoring = {\n",
        "    'accuracy': make_scorer(accuracy_score),\n",
        "    'f1_micro': make_scorer(f1_score, average='micro'),\n",
        "    'precision_macro': make_scorer(precision_score, average='macro'),\n",
        "    'recall_macro': make_scorer(recall_score, average='macro')\n",
        "}\n",
        "\n",
        "rf_cv = RandomForestClassifier(\n",
        "    n_estimators=953,\n",
        "    max_depth=None,\n",
        "    max_features=0.5,\n",
        "    min_samples_split=9,\n",
        "    min_samples_leaf=3,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=2025\n",
        ")\n",
        "\n",
        "cv_results = cross_validate(rf_cv, Xp, y, cv=skf, scoring=scoring, n_jobs=-1)\n",
        "print(\"\\nCross-Validation Metrics (5-fold):\")\n",
        "for metric in scoring.keys():\n",
        "    scores = cv_results[f'test_{metric}']\n",
        "    print(f\" - {metric}: {scores.mean():.4f} Â± {scores.std():.4f}\")"
      ],
      "metadata": {
        "id": "ZW4yYtXP4K86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 6: Model Training with Progress"
      ],
      "metadata": {
        "id": "aIU5CP1D4f-4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "n_total = 953\n",
        "chunk = 100\n",
        "rf = RandomForestClassifier(\n",
        "    n_estimators=0,\n",
        "    warm_start=True,\n",
        "    max_depth=None,\n",
        "    max_features=0.5,\n",
        "    min_samples_split=9,\n",
        "    min_samples_leaf=3,\n",
        "    bootstrap=True,\n",
        "    n_jobs=-1,\n",
        "    random_state=2025\n",
        ")\n",
        "\n",
        "cum = 0\n",
        "while cum < n_total:\n",
        "    add = min(chunk, n_total - cum)\n",
        "    rf.set_params(n_estimators=cum + add)\n",
        "    rf.fit(Xp, y)\n",
        "    cum += add\n",
        "    pct = 30 + int(cum / n_total * 60)\n",
        "    progress(pct)\n",
        "\n",
        "progress(90)\n"
      ],
      "metadata": {
        "id": "5Uka5Bsv4j_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 7: Predict & Save"
      ],
      "metadata": {
        "id": "H3OJ4dOh4ncS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pred = rf.predict(Xt_p)\n",
        "pd.DataFrame(pred, index=Xt.index, columns=['damage_grade']).to_csv(BASE / 'RF_preprocessed.csv')\n",
        "\n",
        "progress(100)\n",
        "print(\"File 'RF_preprocessed.csv' salvato\")\n"
      ],
      "metadata": {
        "id": "Yg_fQUds4qyH"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}